# -*- coding: utf-8 -*-
"""transformer-text8.ipynb

Automatically generated by Colaboratory.

"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
from torch.nn import Transformer

# Load the dataset
def load_text8(file_path, train_size=1000000, val_size=100000, test_size=100000):
    with open(file_path, 'r') as f:
        text = f.read()
    
    train_data = text[:train_size]
    val_data = text[train_size:train_size + val_size]
    test_data = text[train_size + val_size:train_size + val_size + test_size]

    return train_data, val_data, test_data

from google.colab import drive

drive.mount('/content/drive')

# Preprocess the dataset
class Text8Dataset(Dataset):
    def __init__(self, data, seq_len):
        self.data = data
        self.seq_len = seq_len
        self.vocab = sorted(list(set(data)))
        self.vocab_size = len(self.vocab)
        self.char_to_ix = {ch: i for i, ch in enumerate(self.vocab)}
        self.ix_to_char = {i: ch for i, ch in enumerate(self.vocab)}

    def __len__(self):
        return len(self.data) - self.seq_len

    def __getitem__(self, index):
        input_seq = self.data[index:index + self.seq_len]
        target_seq = self.data[index + 1:index + self.seq_len + 1]
        input_seq_encoded = [self.char_to_ix[ch] for ch in input_seq]
        target_seq_encoded = [self.char_to_ix[ch] for ch in target_seq]
        return torch.tensor(input_seq_encoded), torch.tensor(target_seq_encoded)

# Define the Transformer model
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class TransformerModel(nn.Module):
    def __init__(self, ntoken, d_model, nhead, nhid, nlayers, dropout=0.5):
        super(TransformerModel, self).__init__()
        self.model_type = 'Transformer'
        self.src_mask = None
        self.pos_encoder = PositionalEncoding(d_model, dropout)
        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, nhid, dropout)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)
        self.encoder = nn.Embedding(ntoken, d_model)
        self.d_model = d_model
        self.decoder = nn.Linear(d_model, ntoken)

        self.init_weights()

    def _generate_square_subsequent_mask(self, sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def init_weights(self):
        initrange = 0.1
        self.encoder.weight.data.uniform_(-initrange, initrange)
        self.decoder.bias.data.zero_()
        self.decoder.weight.data.uniform_(-initrange, initrange)

    def forward(self, src):
        if self.src_mask is None or self.src_mask.size(0) != len(src):
            device = src.device
            mask = self._generate_square_subsequent_mask(len(src)).to(device)
            self.src_mask = mask

        src = self.encoder(src) * np.sqrt(self.d_model)
        src = self.pos_encoder(src)
        output = self.transformer_encoder(src, self.src_mask)
        output = self.decoder(output)
        return output

def train(model, dataloader, criterion, optimizer, device):
    model.train()
    total_loss = 0.0
    correct = 0
    total = 0
    for batch_idx, (inputs, targets) in enumerate(dataloader):
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)

        loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        _, predicted = outputs.max(2)
        total += targets.size(0) * targets.size(1)
        correct += predicted.eq(targets).sum().item()

    return total_loss / (batch_idx + 1)

def evaluate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
      for batch_idx, (inputs, targets) in enumerate(dataloader):
          inputs, targets = inputs.to(device), targets.to(device)
          outputs = model(inputs)

          loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))
          total_loss += loss.item()
          _, predicted = outputs.max(2)
          total += targets.size(0) * targets.size(1)
          correct += predicted.eq(targets).sum().item()

    return total_loss / (batch_idx + 1)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
train_data, val_data, test_data = load_text8('/text8/text8')
seq_len = 128

train_dataset = Text8Dataset(train_data, seq_len)
val_dataset = Text8Dataset(val_data, seq_len)
test_dataset = Text8Dataset(test_data, seq_len)

train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)
test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)

vocab_size = train_dataset.vocab_size
d_model = 512
nhead = 8
nhid = 2048
nlayers = 6

print(vocab_size)

model = TransformerModel(vocab_size, d_model, nhead, nhid, nlayers).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)

print(model)

scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=3)

num_epochs = 50
best_val_loss = float('inf')
train_loss_history = []
val_loss_history = []
for epoch in range(num_epochs):
    train_loss = train(model, train_dataloader, criterion, optimizer, device)
    val_loss = evaluate(model, val_dataloader, criterion, device)

    print(f"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
    train_loss_history.append(train_loss)
    val_loss_history.append(val_loss)
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(model.state_dict(), '/model/model.pth')
    
    scheduler.step(val_loss)

import matplotlib.pyplot as plt
plt.figure(figsize=(10, 5))
plt.plot(train_loss_history, label='Training Loss')
plt.plot(val_loss_history, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

model.load_state_dict(torch.load('/model/model.pth'))
test_loss, test_acc = evaluate(model, test_dataloader, criterion, device)
print(f"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}")

def generate_text(model, start_seq, max_length=100, k=5, device='cpu'):
    model.eval()
    model.to(device)
    
    input_sequence = start_seq
    generated_sequence = start_seq

    with torch.no_grad():
        for _ in range(max_length):
            input_tensor = torch.tensor([train_dataset.char_to_ix[ch] for ch in input_sequence], dtype=torch.long).to(device)
            input_tensor = input_tensor.unsqueeze(1)

            output = model(input_tensor)
            output = output[-1, :]
            probabilities = torch.softmax(output, dim=-1)

            # Apply Top-k sampling
            top_k_values, top_k_indices = torch.topk(probabilities, k)
            next_token = top_k_indices[0, torch.multinomial(top_k_values, num_samples=1)].item()

            next_char = train_dataset.ix_to_char[next_token]
            generated_sequence += next_char
            input_sequence = input_sequence[1:] + next_char

    return generated_sequence



model.load_state_dict(torch.load('/model/model.pth'))
start_seq = 'the means of production '
generated_text = generate_text(model, start_seq, max_length=100, k=5, device='cpu')
print(generated_text)

